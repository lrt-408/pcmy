{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "（通讯员张宇锋）10月26日下午，农业农村部原副部长、国务院参事室参事于康震在吉林省畜牧业管理局、延边州农业农村局相关负责人的陪同下来我校调研指导工作，校党委书记冯涛、副校长王洪福迎接了于康震一行并陪同调研。\n",
      "（出版社供稿） 10月25日上午，校党委书记冯涛，校党委副书记、校长蔡红星一行到延边大学出版社有限责任公司（以下简称“出版社”）长春分部调研指导工作。\n",
      "（马克思主义学院供稿）10月17日，由吉林省教育厅高校思政处组织的高校马克思主义学院院长论坛在我校举办。吉林省教育厅高校思政处处长杨延秋、吉林省高校网络思想政治工作中心执行主任王佳惠出席论坛。\n",
      "（马克思主义学院供稿）10月13日至17日，全省高校马克思主义学院院长和中小学思政课骨干教师实践研修班在全国高校思想政治理论课教师研修基地（延边大学）（以下简称“延边大学基地”）举办，吉林省教育厅高校思政处处长杨延秋,校党委副书记于金欢出席开班仪式。\n",
      "（通讯员张宇锋）10月26日下午，农业农村部原副部长、国务院参事室参事于康震在吉林省畜牧业管理局、延边州农业农村局相关负责人的陪同下来我校调研指导工作，校党委书记冯涛、副校长王洪福迎接了于康震一行并陪同调研。\n"
     ]
    }
   ],
   "source": [
    "#网络连接\n",
    "import requests\n",
    "#提取内容\n",
    "import re\n",
    "#接口统一\n",
    "from abc import ABC, abstractmethod\n",
    "import time \n",
    "import random\n",
    "\n",
    "class BaseSpyder:\n",
    "    def __init__(self, url, mark):\n",
    "        self.header = {\n",
    "            'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36 Edg/118.0.2088.69'\n",
    "        }\n",
    "        self.url = url\n",
    "        self.mark = mark\n",
    "    #判断网站是否能顺利打开\n",
    "    def response(self):\n",
    "        if requests.get(self.url, headers=self.header).status_code == 200:\n",
    "            return requests.get(self.url, headers=self.header)\n",
    "        else :\n",
    "            return False\n",
    "#接口\n",
    "class GetNewContent(ABC):\n",
    "    @abstractmethod\n",
    "    def getTitles(self, data):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def getContents(self, data):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def getUrls(self, data):\n",
    "        pass\n",
    "\n",
    "#获取新闻列表中的前5条新闻标题和相关连接。继续访问新闻链接获取其中的新闻内容\n",
    "class MySpyder(BaseSpyder,GetNewContent):\n",
    "    def __init__(self, url, mark):\n",
    "        #覆盖父类方法：\n",
    "        super().__init__(url, mark)\n",
    "\n",
    "    #获取5条新闻标题    \n",
    "    def getTitles(self):\n",
    "        #解决乱码问题\n",
    "        text = self.response().content.decode('utf-8')\n",
    "        if text == False:\n",
    "            print(\"不能成功加载网页\")\n",
    "            return\n",
    "        part = re.compile(r'(?<=<a href=\")info.+?(?=\")')\n",
    "        list1 = part.findall(text)\n",
    "        #print(list1)\n",
    "        cont = 0\n",
    "        for i in list1:\n",
    "            #发送一个HTTP GET请求，获取响应的原始字节内容，并将其解码为Unicode字符\n",
    "            tmp = requests.get(url+'/'+i,headers=self.header).content.decode('utf-8')\n",
    "            #print(tmp)\n",
    "            mytitle = re.compile(r'<b>([\\u4e00-\\u9fa50-9,.，。、“”]+?)</b>')\n",
    "            list2 = mytitle.findall(tmp)\n",
    "            #print(list2)\n",
    "            for j in list2:\n",
    "                print(j)\n",
    "                cont += 1\n",
    "                if cont == 5:\n",
    "                    break\n",
    "            if cont == 5:\n",
    "                break\n",
    "            #print(a2_list)\n",
    "\n",
    "    def getContents(self):\n",
    "        text = self.response().content.decode('utf-8')\n",
    "        if text == False:\n",
    "            print(\"不能成功加载网页\")\n",
    "            return\n",
    "        part = re.compile(r'(?<=<a href=\")info.+?(?=\")')\n",
    "        list1 = part.findall(text)\n",
    "        #print(list1)\n",
    "        cont = 0\n",
    "        for i in list1[:20]:\n",
    "            #发送一个HTTP GET请求，获取响应的原始字节内容，并将其解码为Unicode字符\n",
    "            tmp = requests.get(url+'/'+i,headers=self.header).content.decode('utf-8')\n",
    "            #print(tmp)\n",
    "            mycontents = re.compile(r'<p class=\"vsbcontent_start\" style=\"text-align: left;\">([\\u4e00-\\u9fa5（）0-9《》\\s,.，。、“”]+?)</p>')\n",
    "            list2 = mycontents.findall(tmp)\n",
    "            #print(list2)\n",
    "            for j in list2:\n",
    "                print(j)\n",
    "                cont += 1\n",
    "                if cont == 5:\n",
    "                    break\n",
    "            if cont == 5:\n",
    "                break\n",
    "           \n",
    "\n",
    "    def getUrls(self):\n",
    "        text = self.response().content.decode('utf-8')\n",
    "        if text == False:\n",
    "            print(\"不能成功加载网页\")\n",
    "            return\n",
    "        part = re.compile(r'(?<=<a href=\")info.+?(?=\")')\n",
    "        list1 = part.findall(text)\n",
    "        #print(list1)\n",
    "        cont = 0\n",
    "        for i in list1:\n",
    "            print(url+'/'+i+'/')\n",
    "            cont += 1\n",
    "            if cont == 5:\n",
    "                    break\n",
    "           \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://ybu.edu.cn\"\n",
    "    mark = r'(?<=<a href=\")info.+?(?=\")'\n",
    "    obj = MySpyder(url, mark)\n",
    "    #obj.getTitles()\n",
    "    obj.getContents()\n",
    "    #obj.getUrls()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
